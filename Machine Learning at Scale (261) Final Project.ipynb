{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b77a4b39-3403-4a02-95f3-cad7d280345a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 261 Final Project Main Notebook (team_3-1)\n",
    "### Project Title: Turbulent Times - Flight Delay Prediction Tool\n",
    "\n",
    "#### Team Members: Sunny Shin, Courtney Mazzulla, Julian Rippert, Kevin Hoang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a6513c0-6964-4260-bdd5-35bdfc33a2eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Phase Leader Plan\n",
    "The following lists the project owner for each phase who will be overseeing and managing these specific phases of the final project. The team discussed and agreed on the breakout below.  \n",
    "\n",
    "| Phase | Phase Description | Phase Leader | Deliverable | Due Date |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 0 | Finalize Teams | Courtney | N/A | Week 9 - October 29, 2023 |\n",
    "| 1 | Project plan, describe datasets, joins, tasks and metrics | Sunny | Databricks Notebook with Executive Summary | Week 10 - November 5, 2023 |\n",
    "| 2a | EDA, baseline pipeline, scalability, efficiency, distributed/parallel training, and scoring pipeline  | Julian | Databricks Notebook with Executive Summary | Week 12 - November 29, 2023 |\n",
    "| 2b | In-class presentation | Sunny | PowerPoint | Week 12 - December 3, 2023 |\n",
    "| 3a | Select the optimal algorithm, fine-tune and submit a final report | Kevin | Databricks Notebook with Executive Summary | Week 14 - December 16, 2023 |\n",
    "| 3b | In-class presentation | Julian | PowerPoint | Week 14 - December 13, 2023 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66493f97-fd9c-4735-918e-f9d826ae71e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Credit Assignment Plan\n",
    "The following is a detailed breakdown of phases, tasks, owners, and timelines in a table format.\n",
    "\n",
    "| Phase | Task | Est. Commitment (Person-Hrs) | Owner | Start Date | End Date\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 1 | 1) Phase Leader Plan | .5 | Sunny | 11/1/23 | 11/2/23 |\n",
    "| 1 | 2) Credit Assignment Plan | 3 | Kevin | 11/1/23 | 11/2/23 |\n",
    "| 1 | 3) Project Abstract | 1 | Julian | 11/1/23 | 11/2/23 |\n",
    "| 1 | 4A) Write-up of Data Description | 3 | Courtney | 11/1/23 | 11/4/23 |\n",
    "| 1 | 4B) Data Description - Range and Variable Types EDA | 1.5 | Kevin | 11/2/23 | 11/4/23 |\n",
    "| 1 | 4C) Data Description - Null Analysis and Outliers EDA | 3 | Julian | 10/31/23 | 11/4/23 |\n",
    "| 1 | 4D) Data Description - Normalization EDA | 2 | Sunny | 11/2/23 | 11/4/23 |\n",
    "| 1 | 5) Machine Learning Algorithms and Metrics | 4 | Sunny | 11/2/23 | 11/5/23 |\n",
    "| 1 | 6A) Block Diagram of Machine Learning Pipelines<br />and Responsibilities by Team Member | 2 | Kevin | 11/2/23 | 11/4/23 |\n",
    "| 1 | 6B) Write-up of Planned Pipeline Steps | 1.5 | Julian | 11/2/23 | 11/4/23 |\n",
    "| 1 | 7) Write-up of Conclusion, Next Steps, and Open Issues | 1 | Courtney | 11/2/23 | 11/4/23 |\n",
    "| 1 | 8) Finalize and Submit Notebook | 1 | Sunny | 11/4/23 | 11/5/23 |\n",
    "| 2 | 1) Phase Leader Plan | .5 | Sunny | 11/6/23 | 11/6/23 |\n",
    "| 2 | 2) Credit Assignment Plan | 1 | Kevin | 11/6/23 | 11/6/23 |\n",
    "| 2 | 3A) Full EDA of Airline Data | 10 | Julian | 11/6/23 | 11/10/23 |\n",
    "| 2 | 3B) Full EDA of Weather Data | 10 | Courtney | 11/6/23 | 11/10/23 |\n",
    "| 2 | 3C) Full EDA of Weather Stations Data | 5 | Sunny | 11/6/23 | 11/10/23 |\n",
    "| 2 | 3D) Full EDA of Airport Codes Data | 1 | Kevin | 11/6/23 | 11/10/23 |\n",
    "| 2 | 4) EXTRA CREDIT: Join All Tables for the Entire Data | 20 | Kevin | 11/11/23 | 12/5/23 |\n",
    "| 2 | 5) Full EDA on Joined Dataset, 3 Months and 12 Months | 4 | Kevin | 11/14/23 | 11/16/23 |\n",
    "| 2 | 6A) List of Raw/Derived Model Features | 3 | Kevin | 11/16/23 | 11/17/23 |\n",
    "| 2 | 6B) Implement Needed Dimensionality Reduction Steps | 1.5 | Julian | 11/16/23 | 11/17/23 |\n",
    "| 2 | 6C) Write-up and Implementation of Feature Transformations | 1.5 | Julian | 11/17/23 | 11/18/23 |\n",
    "| 2 | 6D) Partition Cross-Validation Folds, and Produce Training,<br />Validation, and Test Splits | 10 | Kevin | 11/17/23 | 11/18/23 |\n",
    "| 2 | 7A) Write Code for Regression Baseline Pipeline | 8 | Sunny | 11/18/23 | 11/20/23 |\n",
    "| 2 | 7B) Write Code for Ensemble Baseline Pipeline (Random Forest) | 8 | Julian | 11/18/23 | 11/20/23 |\n",
    "| 2 | 7C) Fine Tune Baseline Pipelines Through a Grid Search<br />and Select Best Hyperparameters | 8 | Sunny,<br />Kevin | 11/20/23 | 11/22/23 |\n",
    "| 2 | 8) Create at Least 1 Time-Based Feature to Improve Loss by<br />>= 3% Change | 6 | Kevin | 11/20/23 | 11/21/23 |\n",
    "| 2 | 9) Experimentation of Baselines on 1 Year Joined Dataset<br />to Compute Metrics Described in Phase 1 | 4 | Courtney,<br />Julian | 11/22/23 | 11/26/23 |\n",
    "| 2 | 10A) In-Class Presentation Slides | 3 | Courtney | 11/27/23 | 11/27/23 |\n",
    "| 2 | 10B) In-Class Presentation Dry Run | 0.5 | All | 11/27/23 | 11/28/23 |\n",
    "| 2 | 11) Finalize and Submit Notebook | 2 | Courtney | 11/28/23 | 12/3/23 |\n",
    "| 3 | 1) Phase Leader Plan | .5 | Sunny | 11/30/23 | 11/30/23 |\n",
    "| 3 | 2) Credit Assignment Plan | 1 | Kevin | 11/30/23 | 11/30/23 |\n",
    "| 3 | 3A) Write Code for Multi-Layer Perceptron Model | 4 | Kevin | 11/30/23 | 12/2/23 |\n",
    "| 3 | 3B) Fine Tune Deep Learning Models | 12 | Kevin | 12/3/23 | 12/5/23 |\n",
    "| 3 | 3C) Experimentation of Deep Learning Models on all Data<br />and Evaluate Cross-Validation and 2019 Metrics | 12 | Courtney,<br />Kevin | 12/5/23 | 12/7/23 |\n",
    "| 3 | 3D) Write-up: Experimentation Results, Cluster Specs<br />and Times, and Gap Analysis | 3 | Julian | 12/7/23 | 12/8/23 |\n",
    "| 3 | 4A) Feature Engineering: Time-Based Feature | 4 | Julian | 12/3/23 | 12/5/23 |\n",
    "| 3 | 4B) Feature Engineering: Graph-Based Feature | 4 | Kevin | 12/3/23 | 12/5/23 |\n",
    "| 3 | 4C) Feature Engineering: Event-Based Feature | 4 | Sunny | 12/3/23 | 12/5/23 |\n",
    "| 3 | 4D) Feature Engineering: Seasonality Feature | 4 | Courtney | 12/3/23 | 12/5/23 |\n",
    "| 3 | 4E) Rerun Models with New Features | 12 | Sunny | 12/5/23 | 12/7/23 |\n",
    "| 3 | 5) Write-up: Final Conclusions | 4 | Julian | 12/10/23 | 12/10/23 |\n",
    "| 3 | 6A) In-Class Presentation Slides | 3 | Kevin | 12/10/23 | 12/10/23 |\n",
    "| 3 | 6B) In-Class Presentation Dry Run | 0.5 | All | 12/10/23 | 12/11/23 |\n",
    "| 3 | 7) Produce Final Draft of Notebook to Submit | 8 | Kevin | 12/13/23 | 12/15/23 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d21ee0ed-4ae4-4775-9802-5c4043747962",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Abstract\n",
    "Predicting flight delays is valuable in helping airlines optimize operations and curtail frustration toward customers. \n",
    "\n",
    "We developed three regression models to predict the number of minutes a flight will be delayed: linear regression (our baseline), random forest, and XGBoost. The Symmetric Mean Absolute Percentage Error (SMAPE) and Mean Absolute Error (MAE) metrics assess performance of models in both relative and absolute fashions, giving decisive evidence on baseline improvements. Due to the dataâ€™s time-series nature, we implemented time-based features and sliding windows cross-validation to find the optimal model. \n",
    "\n",
    "Our baseline achieved a SMAPE of 82% and a MAE of 31.77 minutes on S2 2018 flights data. Our top model, XGBoost, improved upon baseline to a SMAPE of 80.3% and a MAE of 24.76 minutes on the same dataset. We then applied XGBoost toward blind test data of 2019 flights, achieving similar SMAPE of 80.3% and MAE of 25.74 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60f65dd0-9971-4172-84d3-e2bb8843cc98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Introduction\n",
    "\n",
    "The US commercial airline industry is one of the most vital components of the US economy, contributing to over 5% to the country's GDP at approximately $1.25 trillion in 2022. [1] It not only directly employs millions of people but also indirectly supports countless of others via downstream industries, such as food vendors and travel agencies. Flight delays, while frustrating for passengers, also pose significant financial burdens to airlines, escalating operational costs and increasing the likelihood of subsequent delays.\n",
    "\n",
    "This provides a unique opportunity for airline companies to explore different ways to *accurately* forecast flight delays. Such a model enables the business to take proactive measures to eliminate or minimize disruptions and to enhance the overall travel experience for the customers, thereby differentiating themselves from its competitors.\n",
    "\n",
    "As independent consultants and Machine Learning practitioners, we plan to tackle this opportunity through an ensemble-based modeling approach trained on time-series historical flight data from 2015 to 2019, with a focus on domestic US flights only. The success of our model will be measured by various metrics including Symmetric Mean Absolute Percentage Error (SMAPE) to assess the accuracy of the model's flight delay predictions.\n",
    "\n",
    "Our flight delay prediction model is intended to provide value-add for both airlines and its customers such as: \n",
    "* Improved customer satisfactionÂ \n",
    "* Efficient resource (staffing, gates, aircrafts) allocation for airlines\n",
    "* Competitive advantage - Airlines with accurate flight delay prediction would be a leg-up on any airline within the current marketplace\n",
    "\n",
    "**Outcome:**\n",
    "\n",
    "The difficulty of predicting flight delays is that there are many potential ways to define delay via our available dataset. We plan to provide the consumers with an predition of the potential delay in number of minutes in which particular flight would likely be delayed. \n",
    "\n",
    "We plan to conduct this modeling activity exploring the DEP_DELAY column.\n",
    "\n",
    "The two main metrics we plan to compare the performance of different regression models are the Symmetric Mean Absolute Percentage Error (SMAPE) and Mean Absolute Error (MAE). \n",
    "\n",
    "The formula for SMAPE is as follows:\n",
    "\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/phase_3_smape_2.png?raw=true\" width=350>\n",
    "\n",
    "where \\\\(n\\\\) is the number of times the summation iteration happens, \\\\(A{_t}\\\\) is for actual value and \\\\(F{_t}\\\\) is the forecast values. SMAPE measures the average distance between model predictions and actual values, as a percentage of the sum of prediction and actual values. It's a metric commonly used to evaluate the accuracy of a forecasting or prediction model. Unlike the traditional Mean Absolute Percent Error (MAPE), SMAPE allows us to address the issue where MAPE can become undefined or extremely large if the actual values are 0 or very close to 0. It's very useful in business and finance settings due to how easy it is to interpret the metrics. It is also scale-independent, meaning it can be used to compare the accuracy of models across different datasets or variables, and it's not affected by the scale of the data.  \n",
    "\n",
    "One of the key traits of SMAPE is that it is less sensitive to ouliers than other similar metrics because by using percentages, SMAPE normalizes the errors relative to the magnitude of the actual values. This means that larger errors in predictions for large actual values contribute proportionally as much as small errors in predictions for small actual values. However, SMAPE can be problematic when actual values are close to or eqaul to zero, as division by small values can cause skewness in the results. \n",
    "\n",
    "The second metric we used, MAE, is calculated as follows:\n",
    "\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/mae.png?raw=true\" width=280>\n",
    "\n",
    "where \\\\(n\\\\) is the number of times the summation iteration happens, \\\\(\\hat{y_i}\\\\) is the predicted value of the \\\\(i{^{th}}\\\\) observation and \\\\(y{_i}\\\\) is the actual value of the \\\\(i{^{th}}\\\\) observation. It measures the average distance between predicted and actual values in terms of minutes, and gives us a sense of how much our predictions differ from actual values in an absolute sense, in the case that values near zero in the SMAPE formula are harshly penalizing us. \n",
    "\n",
    "[1] https://www.airlines.org/impact/#:~:text=Economic%20Impact%20Of%20Commercial%20Aviation&text=Commercial%20aviation%20drives%205%25%20of,from%20more%20than%20220%20countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a7223fd-1714-417d-b006-fa19ed05f040",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "In this section, we delve into the datasets utilized for our analysis, elucidating its sources and temporal scope. Further in our report, we articulate data format, variables, and inherent modeling limitations of the data given. At the project's inception, we were furnished with several datasets, including:\n",
    "\n",
    "*Raw Unprocessed Flight Delays Table:*\n",
    "1. Flights data spanning from 2015 to 2019, offering raw information on flights, sourced from the US Department of Transportation. The dataset encompasses three months, six months, one year, and full data from 2015 to 2019.\n",
    "2. Weather data from 2015 to 2019, providing raw weather information, sourced from National Oceanic and Atmospheric Administration. Similar to the flights dataset, it comprises varying temporal scopes.\n",
    "3. Stations dataset, containing station ID, latitude, longitude, and the distance between the base station and nearby stations, sourced from the US Department of Transportation.\n",
    "4. Airport codes dataset, originating from the public domain source ourairports.com.\n",
    "\n",
    "*Airport Codes Table On-Time Performance of Flights and Weather Dataset Joined (OTPW):*\n",
    "1. OTPW, denoting Arrival Time Performance with Local Weather data, represents the amalgamation of flights, weather, stations, and airport datasets from 2015-2019. This comprehensive dataset was the primary resource during the initial data exploratory phase.\n",
    "\n",
    "During the exploratory data analysis, our team predominantly leveraged the OTPW dataset. However, a deliberate effort was made to independently process and join all raw datasets into a custom amalgamated dataset, facilitating further exploration with diverse variables. We are pleased to announce that our effort of constructing our own OTPW table and joining all the raw data from the given data sources was successful. While the OTPW dataset served as the cornerstone for initial exploration, the creation of this custom joined dataset enables us to explore varied features in subsequent stages, enhancing the robustness of our analytical models. Our results and analysis will be driven off of the full joined 2015-2019 OTPW dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4edc2791-8070-40da-9a4a-f2212fb2b660",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Full Join of the OTPW Data (Extra Credit)\n",
    "The step by step process we followed to join the data is as follows: \n",
    "1. Begin with joining the stations dataset to the airport codes dataset filtered to rows with an IATA code, to associate each IATA code with a corresponding weather station ID.\n",
    "2. Join flights onto this joined stations and airports table via origin IATA code.\n",
    "3. Use a time zone mapping table to map each IATA code to a time zone, and convert scheduled departure time to UTC (as the weather table is in UTC according to the [*documentation*](https://www.ncei.noaa.gov/data/global-hourly/doc/isd-format-document.pdf) by the National Centers for Environmental Information).\n",
    "4. Join the weather data onto the joined flights, stations, and codes table via station ID and onto a -4 to -2 hour advance window to the UTC scheduled departure time, and take the earliest weather reading in the window.\n",
    "\n",
    "In more detail, we lay out our steps, dataset sizes, and join times. Join times are reported in terms of the next action onto the dataframe that is called, as all join calls, even the most complex ones, take less than a second, and are only manifested when an action like writing to parquet or displaying is called.\n",
    "\n",
    "**All joins and computations** described in this section were computed using **1-4 workers on 4 cores and 14GB memory.** The number of workers was variable across different sessions as we logged in throughout multiple days to test and complete this join process.\n",
    "\n",
    "####*1. Stations to airport codes.*\n",
    "\n",
    "There are 9,225 rows and 12 columns in the airport codes table and 5,004,169 rows and 12 columns in the stations table. We first take a minor processing step of separating out coordinates into latitude and longitude in the codes table. However, the stations table consists of station pairs, and since we don't need station pairs, we dedupe by just taking rows in the stations table where the station equals its neighbor - this reduced the dataset down to 2,198 rows. Upon even further analysis, under the neighbor_call column (which contains ICAO code) there are 16 entries associated with duplication: 8 codes that occur twice. For these 8 codes, we take only the station ID that is associated with the title having the word \"airport\", as the other station ID ties to another local station name.\n",
    "<br></br>\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/phase_3_team_join_station_dupes.png?raw=true\" width=700>\n",
    "\n",
    "Once we correct for this duplication, we perform a join onto the airport codes table where the airport codes **gps_code** column matches the **neighbor_call** stations column (both are essentially ICAO codes). The stations table has redundant information with the codes table such as coordinates while the neighbor information is unnecessary, so we only take the neighbor_call and station_id columns here. We also remove unnecessary columns from the codes table and only select the following columns: gps_code, iata_code, local_code, name, type, iso_region, longitude, latitude, and elevation_ft.\n",
    "\n",
    "The resulting inner join in this step took 3.5 seconds and yielded 1,433 rows and 11 columns.\n",
    "\n",
    "One final call-out in this step: this join did not encapsulate all IATA codes that occurred in the entire flights dataset. There were 8 missing IATA codes from this first join: PSE, PPG, SJU, GUM, OGS, SPN, XWA, TKI. All codes except OGS correspond to airports in the U.S. territories, and the reason why these were missed was that their corresponding ICAO code was not present in the stations table. Instead, for all of these 8 IATA codes except GUM and SPN, we take the *closest* station - the exclusion for GUM and SPN is due to their closest station being too far away from the airports, so we note that these IATA codes are dropped completely from the full join.\n",
    "\n",
    "We then join the station IDs for the closest station to these remaining 6 IATA codes (2.2 second join), and union them back into the first joined table to yield 1,439 rows and 11 columns on this joined codes and stations data. Going forward, we'll call this the **\"CS\" table** for brevity.\n",
    "<br></br>\n",
    "*CS Output Snippet*\n",
    "\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/phase_3_team_join_codes_stations.png?raw=true\" width=900>\n",
    "\n",
    "####*2 & 3. Joined Stations + Airport Codes onto Flights, Time Conversion to UTC.*\n",
    "\n",
    "In this next step, we first map time zones onto the joined CS data via the table at the following github link: https://raw.github.com/hroptatyr/dateutils/tzmaps/iata.tzmap. Certain obscure codes including HGT, PLB, and CGX were absent from this table, and here I mapped time zones manually off of a google search of the city the airports were located at and then the city's time zone.\n",
    "\n",
    "This time zone table consists of 9,067 rows and 2 columns. The CS table was left joined onto this table on **IATA code** (a 2.32 second join) to yield a 1,439 row by 13 column table.\n",
    "\n",
    "Then, we bring in the entirety of the flights table, at 74,177,433 rows by 109 columns. To facilitate efficient joins and prevent doing a large join that compares all records from the flights and eventually the weather tables, we partition the flights table into yearly chunks. In each yearly chunk, we left join the flights table onto CS table two times: the first time left joining onto the CS table on flight **Origin IATA** to CS **IATA** and the second time on flight **Destination IATA** to CS **IATA**. In the joins, we obtain ICAO, IATA, airport name, type, region, latitude, and longitude for both origin and destination, and additionally, elevation, station ID, and time zone for origin only to join onto the weather data. We use a SQL expression to create a full scheduled departure time value, convert the scheduled departure time to UTC with this joined time zone, and produce two more records for two hours prior and four hours prior to the UTC scheduled departure time for joining onto the weather table.\n",
    "\n",
    "In addition, we note the existence of duplicated flights across the same tail number and scheduled flight time. We dedupe across tail number and scheduled flight time in UTC and checkpoint our deduped data by writing each yearly chunk to parquet files. We performed the two joins of flights data onto the CS table, produced timestamps, and deduped all in succession before writing to parquet, and this step in totality took 29.85 minutes, writing a total of 31,247,091 rows and 130 columns of data to team storage. We'll call this table the **\"FT\" data** for brevity in the next step.\n",
    "<br></br>\n",
    "*FT Output Snippet*\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/phase_3_team_join_flights_joined_od_times.png?raw=true\" width=1000>\n",
    "\n",
    "####*4. Weather onto Joined Flights + Stations + Airport Codes.*\n",
    "\n",
    "In this final component of our process, we bring in the entirety of the weather data, at 898,983,399 rows by 124 columns. First, we note massive time savings potential by reducing this dataset to only those station codes that occur among IATA codes in the flights table. Namely, we take all origin and destination IATA codes in the flights table, filter the station codes in the CS table only to these IATA codes, and filter the weather data down to only these filtered stations in the CS table.\n",
    "\n",
    "Similarly to the flights data, we split the weather data into yearly chunks.\n",
    "\n",
    "Then, to improve the join times even further, we selected only the hourly measures, as most other columns in the weather table consist of daily or weekly rollups of data which we do not need for our join. We also removed the following columns due to excessive (>50%) null values even after filtering down to stations that occur in the flights dataset: HourlyPresentWeatherType, HourlyPressureChange, HourlyPressureTendency, HourlyWindGustSpeed, HourlyWindGustSpeed. The variable HourlySkyConditions is a text variable with several codes which we do not parse in our study. All other hourly variables are kept.\n",
    "\n",
    "We also dedupe the weather data across the same station and time reading.\n",
    "\n",
    "The bulk of our join time happens in this next step. In fact, because this calculation takes so long, we do another split of each year's data into two semesters using July 1st as the split point. For each year of data, we left join the year's FT data, onto the year's filtered weather data on matching **stations** and where the **time** from the weather table is **greater than the four hours prior departure UTC timestamp and less than the two hours prior departure UTC timestamp** from the FT data. Furthermore, within the same tail number and UTC timestamp, we rank all joined rows from the weather data in ascending order of weather data timestamp, and retain only the top-ranked (i.e. the earliest) reading. Once we complete this filter step, we checkpoint each half of each year's data by writing to parquet. The join times reflect the process of filtering the weather data, splitting, joining onto the two hour window, ranking times, and filtering to the top ranked timestamp all in succession.\n",
    "\n",
    "| Year | Rows in Filtered Joined Flights (FT) Data | Rows in Filtered Weather Data | Join Time |\n",
    "| --- | --- | --- | --- |\n",
    "| 2015 | 5,727,720 | 4,994,133 | 51.48 min |\n",
    "| 2016 | 5,550,490 | 4,905,338 | 45.02 min |\n",
    "| 2017 | 5,590,414 | 4,758,154 | 51.07 min |\n",
    "| 2018 | 7,094,482 | 4,756,117 | 59.90 min |\n",
    "| 2019 | 7,283,985 | 5,030,634 | 31.09 min |\n",
    "\n",
    "In totality, this process took 3 hours and 58.6 minutes, which we ran overnight. We acknowledge that some rows (less than 30) dropped out of the joins for each year of data, and we observed an odd phenomenon re-running the joins and seeing different row counts across runs. We acknowledge a gap in this step, but given that so little data has dropped out, we are comfortable proceeding with this miniscule gap in data.\n",
    "\n",
    "Once we write all the halves of all years of joined data to parquet, we then read all these components and union them into a full 5 year dataset, ending this elaborate join process. This final union and checkpoint of the 5 year data took about 8.5 minutes. Our OTPW 5 year dataset consists of 31,247,069 rows by 143 columns.\n",
    "<br></br>\n",
    "*OTPW 5Y Output Snippet*\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/phase_3_team_join_full.png?raw=true\" width=1000>\n",
    "\n",
    "There are two source notebooks for this task. Please see the one for testing on [*3 months and 2019*](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2798664673110762/command/1012234209195181), and the other [*executing the join on 2015-2018*](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1537154891794676/command/546615846963071) for additional details and code references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4edf4b4a-49eb-4611-9c74-5625fd726058",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Summary of EDA and Feature Engineering\n",
    "\n",
    "At a high level, we do initial EDA work using flights and weather data from 2015 to get a sense of the variables we are given. The stations and codes EDA is done on a universal scale, on all stations and codes. We then proceed to using only the joined data for EDA on the 5-year scale on just the 2015-2018 training set, due to sheer volume in having to produce plots on a 5-year scale for the raw flights and weather tables. \n",
    "\n",
    "**Airline Dataset EDA** \n",
    "\n",
    "The Airline dataset is a large dataset consisting of 109 columns and 2,806,942 rows, sourced from the US Department of Transportation. Each observation in this dataset represent a domestic US Flight within 2015. \n",
    "\n",
    " *Nulls:* The dataset also consist of many columns which are primarily (over 50%) made of null values. Such columns are for instance related to divereted flight. Such occurances are rare which is resulted in the vast null prevelance for such columns. (See below graph, right most columns are all almost nulls)\n",
    "\n",
    " <img src=\"https://github.com/sunnyshin0824/db_test/blob/main/airline_eda_null.png?raw=true\" width=700>\n",
    "\n",
    " For columns which remain after dropping columns which had greater than 50% null prevelance, we conducted a median impution approach for null for our continuous variables. \n",
    "\n",
    "*Range Analysis:* Below, we conducted an range analysis into some of the numerical columns within the dataset. Due to the vast differerences in scale of the columns, we will have to explore a normalization of our features. \n",
    "\n",
    " <img src=\"https://github.com/sunnyshin0824/db_test/blob/main/airline_eda_numeric_range.png?raw=true\" width=700>\n",
    "\n",
    " After further analysis our of categorical columns, we noted an even distirbution of several key temporal related features such as the days of the month, days of the week, etc. See the source notebook for additional details. \n",
    "\n",
    "*Outlier Analysis:* An outlier analysis helped reveal key information of our dataset and how it relates to the target label. We found that after identifying outliers amongst the features, removing those observations resulted in removing all instances of flight delays being true. See below two example columns which had a high presences of outliers. \n",
    "\n",
    "  <img src=\"https://github.com/sunnyshin0824/db_test/blob/main/airline_eda_outlier_analysis.png?raw=true\" width=500>\n",
    "\n",
    "For categorical columns, further analysis showed there are many redundant columns within the dataset which we can go on to remove prior to any model training. For instance, categorical columns such as DEST and DEST_CITY_NAME provide the same information. As such after the removal of redundant cetegorical features, these remaining categorical features were one hot encoded. \n",
    "\n",
    "Please refer to the [*Source Notebook*](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3865911540703561/command/3865911540703562) for additional details and code references.\n",
    "\n",
    "**Weather Dataset EDA**\n",
    "\n",
    "The Weather dataset is a large dataset consisting of 124 columns and 30,528,602 rows, sourced from the National Oceanic and Atmospheric Administration repository. Each observation in this dataset provides a synopsis of climatic values for a single weather station over a specific month within 2015. Geographic availability includes thousands of locations worldwide. Climatic values given include hourly, daily, and monthly measurements of temperature, dew point, humidity, winds, sky condition, weather type, atmospheric pressure and more.\n",
    "\n",
    "*Documentation Review:* The Weather data presents an interesting structure since *each dayâ€™s hourly observations, the daily data for that day is given in the following row and monthly data is in the row following the final hourly observations for the last day in the month*.[2] Daily and monthly values are as described above. After reviewing documentation, business need, and the features, we will only focus on hourly data within the dataset, given it is the smallest granualrity and our analysis will require this. All other columms related to daily and monthly will be dropped from the dataset before performing further analysis.\n",
    "\n",
    "Additionally, for all hourly precipitation values *\"Blank/null = no precipitation was observed/reported for the hour ending at that date/time, while M = missing\"*.[2] For the hourly precipitation metrics, we will note that null values should be considered 0's, while \"M\" values should be considered as normal null values. The rest of the documentation states that *\"M = missing value (appears instead of value), while Blank = value is unreported (appears instead of value)\"*[2], so we will handing the rest of the null values as true nulls.\n",
    "\n",
    "\n",
    "*Nulls:* We conducted null analysis on missing values and nulls to detemrine which of the reamaining features has >50% null values. We will remove these features since there isn't enough data to generalize. For columns which remain after dropping columns which had greater than 50% null prevelance, we conducted a median impution approach for null for our continuous variables.\n",
    "\n",
    "*Data Types:*  After removing unecessary columns, we are left with the following data types and counts:\n",
    "\n",
    "| Data Type | Num. Fields |\n",
    "| --- | --- |\n",
    "| Double | 12 |\n",
    "| String | 8 |\n",
    "\n",
    "*Range Analysis:* Below, basic summary metrics including means, standard deviations and ranges, were computed for these 12 numerical features as shown below. \n",
    "\n",
    " <img src=\"https://github.com/sunnyshin0824/db_test/blob/main/weatherdata_numeric_range_analysis.png?raw=true\" width=900>\n",
    "\n",
    " With context considered, none of these ranges seem too drastic, besides HourlyWindSpeed with a high max outlier value.\n",
    "\n",
    "\n",
    " For the categorical variables, we see that there is some imbalance in the data. We will make sure that we are monitoring these classes closely and interpreting them correctly with the best evaluation metrics and adjusting after baseline metrics are ran.\n",
    "\n",
    " *Outlier Analysis:* An outlier analysis showed us important information about the weather data. Although the data appears to contain outliers for multiple features shown below. However, consdering the context, outliers for weather data can contain valuable information. We will monitor model perfromance and adjust accordingly. We have created two separate parquets for weather data with outliers includes and without. The largest outlier appears to exist for HourlyWindSpeed.\n",
    "\n",
    "  <img src=\"https://github.com/sunnyshin0824/db_test/blob/main/weather_Outlier1.png?raw=true\" width=500>\n",
    "  <img src=\"https://github.com/sunnyshin0824/db_test/blob/main/weather_Outlier2.png?raw=true\" width=1200>\n",
    "  <img src=\"https://github.com/sunnyshin0824/db_test/blob/main/weather_Outlier3.png?raw=true\" width=500>\n",
    "\n",
    "Please refer to the [*Source Notebook*](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2798664673111173) for additional details and code references.\n",
    "\n",
    "**Stations Dataset EDA**\n",
    "\n",
    "The station dataset comprises 12 columns and 5,004,169 rows, sourced from the US Department of Transportation. According to information from the US Department of Transformtation website, the dataset is structured around individual stations, identified by `station_id` and their corresponding `lat`, in short for latitude, and `lon`, in short for longitude coordinates. It captures the distance between the base station and nearby stations, denoted by `neighbor_call` along with their associated `neighbor_lat` and `neighbor_lon`. The `neighbor_call` field, a unique identifier per site, is sourced from the MesoWest Information website, which provides access to current and archived weather observations across the United States. [3] The `distance_to_neighbor` is measured in sm, which stands for the Statute Mile, which is a unit of measurement used in the United States for land-based measurements. It's based on a historical unit of length, the Roman mile, which was defined as 1,000 paces. [4] \n",
    "\n",
    "Once we review all the features in the station dataset, we note that the dataset contains 7 fields in string format and 5 in double format. After a thorough review, it was determined that 9 out of 12 fields are truly numerical variables, prompting their conversion to numerical data types. \n",
    "\n",
    "| Data Type | Num. Fields |\n",
    "| --- | --- |\n",
    "| Double | 9 |\n",
    "| String | 4 |\n",
    "\n",
    "Basic summary metrics including means, standard deviations and ranges, were computed for these 9 numerical features as shown below. \n",
    "\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/stations_summary.png?raw=true\" width=1100>\n",
    "\n",
    "Prior to any feature engineering, graphical representations were generated to understand the data distribution. Of particular interest was the `distance_to_neighbor`, reflecting the distance between the base station, which is noted as `station_id` and its latitude and longtitude coordinates, and the weather station location, which is denoted as `neighbor_call` represented by `neighbor_lat` and `neighbor_lon`. The distribution plot shows that `distance_to_neighbor` is rightly skewed with the median of approximately 900sm from its base station. Per the summary metric above, the average distance to the weather station is 1,343.5sm. \n",
    "\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/stations_distr.png?raw=true\" width=800>\n",
    "\n",
    "The dataset conains 2,237 unique `station_id` spread across 50 different states and 2 territories, including Virgin Islands (VI) and Perto Rico (PR). Notably, there are 2,229 unique site codes, represented in `neighbor_call` nationwide.\n",
    "\n",
    "| Distinct_Station_ID | Distinct_Weather_Site | Distinct_States |\n",
    "| --- | --- | --- |\n",
    "| 2237 | 2229 | 52 |\n",
    "\n",
    "Please refer to the [*Source Notebook*](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1048238643074031/command/1871383191021880) for additional details and code references.\n",
    "\n",
    "**Airport Codes Dataset EDA**\n",
    "\n",
    "The airport codes dataset consists of 57,421 entries and 12 columns, a Datahub dataset originating from the public domain source ourairports.com. We note that the flights dataset has records for origin IATA code and destination IATA code, none of which are null upon our earlier null analysis. Hence, the records in the codes dataset with no IATA code are not relevant in the scope of this project, so we filter down this dataset to entries which the IATA code is not null, at 9,225 rows post-filtering. There is currently one numeric column, which is elevation, but there is a column for coordinates which presents a numeric longitude and then a latitude separated by a comma. We parse longitude and latitude into two additional numeric columns to yield 11 text-based columns and 3 numeric columns.\n",
    "| Data Type | Num. Fields |\n",
    "| --- | --- |\n",
    "| Double | 3 |\n",
    "| String | 11 |\n",
    "\n",
    "The categorical columns mostly consist of identifying information, such as the airport type (large, small, heliport, etc.), GPS code, region, and local code. The below table gives a count of unique entries by column.\n",
    "\n",
    "|ident |type |name |continent |iso_country |iso_region |municipality |gps_code |iata_code |local_code | \n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "|9,225|6|9,150|7|239|2,143|7,776|8,568|9,113|2,946|\n",
    "\n",
    "An inspection of a boxplot for elevation reveals the prevalence of outliers. However, in the main OTPW dataset, we do not modify outliers of elevation from the codes data as we prefer to keep the values from the source data rather than having to impute or drop records.\n",
    "\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/codes_eda_elevation.png?raw=true\" width=800>\n",
    "\n",
    "Longitudes and latitudes do not take extreme values as they are bounded. The dataset has more airport codes with higher latitudes, and the tendency toward higher latitudes will be even more prevalent in the OTPW dataset, as the flights in OTPW are located in the US.\n",
    "\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/codes_eda_coords.png?raw=true\" width=800>\n",
    "\n",
    "Please refer to the [*Source Notebook*](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1871383191023023/command/2798664673120467) for additionals details and code references.\n",
    "\n",
    "\n",
    "[2] https://www.ncei.noaa.gov/pub/data/cdo/documentation/LCD_documentation.pdf\n",
    "\n",
    "[3] https://weather.gladstonefamily.net/site/KJFZ\n",
    "\n",
    "[4] https://www.geeksforgeeks.org/difference-between-nautical-mile-and-statute-mile/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e2e6a00-842e-4681-9d38-5deb5dd741ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### EDA on Joined OTPW 5-Year Dataset\n",
    "\n",
    "##### Data Dictionary and Meta-Information\n",
    "In our efforts to build model baselines, we evaluated the features in the 5-year OTPW joined dataset and arrived at the following data dictionary of 25 fields/columns. We experimented with two target features: shifted departure delay and log transformed shifted departure delay. These are listed first and *italicized*. All subsequent features are input features. This is all prior to feature engineering, which we assess in a later section.\n",
    "\n",
    "| Field Name | Name in OTPW | Data Type |\n",
    "| --- | --- | --- |\n",
    "| *Shifted Departure Delay* | DEP_DELAY_SHIFTED | Int |\n",
    "| *Log Shifted Departure Delay* | DEP_DELAY_SHIFTED_LOG | Double |\n",
    "| Origin Latitude | origin_station_lat | Double |\n",
    "| Origin Longitude | origin_station_lon | Double |\n",
    "| Destination Latitude | dest_airport_lat | Double |\n",
    "| Destination Longitude | dest_airport_lon | Double |\n",
    "| Elevation | ELEVATION | Double |\n",
    "| Hourly Altimeter Setting | HourlyAltimeterSetting | Double |\n",
    "| Hourly Dew Point Temp. | HourlyDewPointTemperature | Double |\n",
    "| Hourly Dry Bulb Temp. | HourlyDryBulbTemperature | Double |\n",
    "| Hourly Precipitation | HourlyPrecipitation | Double |\n",
    "| Hourly Relative Humidity | HourlyRelativeHumidity | Double |\n",
    "| Hourly Sea Level Pressure | HourlySeaLevelPressure | Double |\n",
    "| Hourly Station Pressure | HourlyStationPressure | Double |\n",
    "| Hourly Visibility | HourlyVisibility | Double |\n",
    "| Hourly Wet Bulb Temp. | HourlyWetBulbTemperature | Double |\n",
    "| Hourly Wind Direction | HourlyWindDirection | Int |\n",
    "| Hourly Wind Speed | HourlyWindSpeed | Double |\n",
    "| Day of Month | DAY_OF_MONTH | String |\n",
    "| Day of Week | DAY_OF_WEEK | String |\n",
    "| Airline Carrier | OP_UNIQUE_CARRIER | String |\n",
    "| Month Index | MONTH | String |\n",
    "| Origin Type | origin_type | String |\n",
    "| Destination Type | dest_type | String |\n",
    "| Variable Winds Flag | VariableWinds | Boolean |\n",
    "\n",
    "The columns we did not include either were IDs, had excessive null values, or were on the flights dataset collected post-departure. For example, the wheels off time and arrival delay would present significant leakage concerns being tracked after departure. For the variable winds flag, we created another boolean column that checked whether the \"HourlyWindDirection\" column showed \"VRB,\" denoting variable wind direction.\n",
    "\n",
    "In addition, we omitted columns in which the departure delay evaluated to null, as on inspection we saw that these were associated with cancelled or diverted flights, which we deemed outside the scope of this project to predict the severity of flight delays via a regression model. This left us with 31,242,325 remaining rows of data from our original joined table.\n",
    "\n",
    "##### Features Visualization\n",
    "\n",
    "We scale up our EDA to the 5-year joined dataset. To reiterate, the EDA is conducted on our full 2015-2018 training set to maintain blindness of the 2019 test set. Once again, we plot numeric features in our data dictionary on our own joined OTPW dataset to get a sense of central tendencies and distributions of numeric variables. Like on the 12-month scale, see that most numeric variables follow a roughly symmetric distribution on the 5-year scale, but variables such as Hourly Precipitation and Hourly Visibility appear to have few positive extreme values skewing their distributions to the right.\n",
    "<br/><br/>\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/phase_3_eda_hists.png?raw=true\" width=750>\n",
    "\n",
    "We also examine the categorical variables we included and gauge potential associations with departure delay by plotting averages by category. Our findings from the 12-month dataset in Phase 2 largely apply onto the 5-year scale. We highlight the following categorical variables.\n",
    "- Carrier, where Hawaiian (HA) and Alaskan (AS) yield the lowest average delays (<2.5 min with the others at >5 min). \n",
    "- Month, where the summer months and December have the highest average delays (>10 min). \n",
    "- Origin/destination airport type, where large airports yield the highest average delays (10 min compared to 5-7.5 for medium/small airports).\n",
    "<br/><br/>\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/phase_3_eda_cat.png?raw=true\" width=750>\n",
    "\n",
    "Our initial models in this phase will incorporate all of these features as inputs.\n",
    "\n",
    "Please refer to the [*Source Notebook*](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319525559/command/1346418319525560) for additional details and code references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96ba00fd-57b2-4d8c-9fa4-8ac7078c5325",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Feature Engineering\n",
    "* [Feature Engineering Source Notebook](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1962809137470599/command/1962809137470600)\n",
    "\n",
    "##### Transforming the Target Variable\n",
    "In our feature engineering efforts, we made the decision to first shift our departure delay such that negative delays get mapped to 0, as we view negative delays or early departures as synonymous with no delay. Then, with this shifted delay variable, we also made a log transformation, adding 1 to the shifted delay and taking a logarithm to avoid errors with taking a logarithm of 0.\n",
    "\n",
    "The log transformation smoothed out the extreme values of departure delay to a notable degree, but there is still a large portion of small values for delay, as seen in the peak in the right histogram below.\n",
    "<br></br>\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/phase_3_log_delay.png?raw=true\" width=800>\n",
    "\n",
    "When we ran our models with the log transformed variable, we were experiencing strangely low predictions even after back-transforming (for random forest our predictions onto the pure validation set would have a maximum value of about 10.7), which we theorize to be because the large peak of small values is pulling the model toward very small predictions for the log transformed variable. So, when we back-transform by taking the exponential of the prediction and subtracting 1, we still are left with very low values.\n",
    "\n",
    "Hence, we proceed with having our models predict only the shifted departure delay variable.\n",
    "\n",
    "##### Additional Calculated Features\n",
    "\n",
    "We identified 4 additional features that yielded opportunities to boost the predictive power of our model.\n",
    "1) PageRank score of origin airport: using flights from previous calendar month. To eliminate leakage concerns, for flights occurring on the 1st of a month before 2AM, the pagerank score for that airport is taken from 2 months before. Scores for January 2015 are initialized to 0.\n",
    "2) Previous flight delay of latest flight with same tail number with a scheduled departure time at least 2 hrs prior.\n",
    "3) A flag for whether a flight date is within +/- 5 days of a major U.S. holidays and sporting events (Super Bowl, Daytona 500, NBA+MLB finals, and the U.S. Open).\n",
    "4) Additive seasonality factor, using Prophet to fit a time-series forecasting model on training data and applying that model's predictions to the test data.\n",
    "\n",
    "We also construct some exploratory plots that explore potential associations with the features we made and the target variable of departure delay.\n",
    "\n",
    "For our holidays flag, it appears that the average departure delay time is approximately equal across flights with and without the flag. However, we still view it as worthwhile to include this holiday flag as one of our features in order to capture increased delay times as a result of holiday periods.\n",
    "<br></br>\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/phase_3_fe_holidays.png?raw=true\" width=500>\n",
    "\n",
    "From the scatterplots below, we see interesting relationships with our previous delay feature and departure delay, as we observe a subset of data points that follow a linear relationship in the middle of our previous delay vs. departure delay graph amid many \"noisy\" data points that do not follow a line. However, examining the previous month pagerank feature versus departure delay scatterplot, we see that there are less extreme values the higher the pagerank score increases. This could point to a slight negative correlation between airport pagerank and departure delay.\n",
    "<br></br>\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/phase_3_fe_scatters.png?raw=true\" width=1000>\n",
    "\n",
    "Finally, our methodology in using Prophet was to fit a Prophet model on a training set/fold, predict toward the test fold, and extract the \"additive terms\" in the Prophet model in both the train/test periods to use as a feature. We also see repeating peaks and troughs in average delay as seen in our forecast plot below that could potentially extrapolate to our test sets, which could indicate that the additive terms will be a valuable, unique seasonality feature to improve our model.\n",
    "<br></br>\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/phase_3_fe_prophet.png?raw=true\" width=800>\n",
    "\n",
    "##### Pearson Correlation Analysis\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/otpw_5y_eda_corr.png?raw=true\" width=900>\n",
    "\n",
    "Following our comprehensive feature engineering process, we arrived at a dataset comprising 23 distinct features. Subsequently, we conducted a Pearson correlation analysis to find the relationships among these variables. One intriguing observation was the prevalence of stronger absolute correlations manifesting as inverse or negative correlations, particularly notable in features such as `ELEVATION` and `HourlyStationPressure`. This revelation suggests an intricate interplay between certain atmospheric factors and flight delays. As previously alluded to, we introduced three new features into the dataset, among which the `Prev_Delay` feature emerged as the most noteworthy. Demonstrating a correlation of approximately 0.30, as discernible in the second-to-last row of the correlation heatmap, `Prev_Delay` presents a valuable addition to our predictive model. This positive correlation accentuates the significance of incorporating historical delay data, shedding light on its potential as a robust predictor for future flight delays within our analysis framework.\n",
    "\n",
    "##### Dimensionality Reduction - PCA\n",
    "\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/otpw_5y_eda_pca.png?raw=true\" width=900>\n",
    "\n",
    "Once we completed the feature engineering pipeline, we successfully streamlined our feature space to encompass 23 key variables as mentioned above. During the training of our baseline models, we did not encounter any discernible issues pertaining to extended training times, leading us to forgo the implementation of dimensionality reduction techniques in our existing pipeline. Nevertheless, as a precautionary measure, we conducted a principal component analysis (PCA) exercise to explore the impact of reducing the feature dimensionality. Remarkably, transitioning from 23 to 17 features resulted in a loss of approximately 30% of the explained variance. Although this reduction in explanatory power is noteworthy, we recognize the intrinsic value of PCA, particularly as a prospective tool. Keeping this technique in our repertoire could prove advantageous for future endeavors, such as additional rounds of feature engineering or the assimilation of new datasets. The flexibility offered by PCA ensures its relevance in adapting to evolving analytical needs and enhancing the efficiency of our modeling approach in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdfbc1c4-9589-4a75-8154-dba83cb560f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ee734bc-c3f4-4e07-afd2-8a0ab8d5aa77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Machine Learning Algorithms\n",
    "\n",
    "**Model Pipeline**\n",
    "\n",
    "The following block diagram depicts the detailed machine learning pipeline tasks we executed in this final phase of our project.\n",
    "<br></br>\n",
    "<img src =\"https://github.com/sunnyshin0824/db_test/blob/main/Final%20Project%20-%20Phase%203%20Pipeline%20Diagram.png?raw=true\" width=1000>\n",
    "\n",
    "This pipeline is split into two phases: first, a data preprocessing and preparation phase and second, a model fitting phase. In the preparation phase is where we do our joins, feature engineering, transformations, such as min-max scaling, median imputation, and one-hot encoding, as well as downsampling training folds to achieve a 1 to 1 ratio between delayed flights (>=15 minutes) and non-delayed flights.\n",
    "\n",
    "As usual, we apply transformations such that the values in the training set/fold are used toward the test set/fold to avoid data leakage. We checkpoint our data multiple times in order to test each stage of the preparation process: once after the join, another after feature engineering, and a third time after we partition and preprocess cross-validation and pure/final training and test folds.\n",
    "\n",
    "Then, in the model fitting phase, we develop code for three regression models, use grid search on these models excluding our baseline, and evaluate cross-validation metrics to decide on a best model. In addition, we experiment with a multilayer perceptron model (MLP). Due to technical limitations in getting a regression multilayer perceptron to function, we trained an MLP classification model and used this as a comparison point to gauge how well our regression models would perform in a classification framework: if we were to classify delay/no delay based on whether a regression model predicts a delay time of at least 15 minutes, how accurate would the classifications be? Though our main objective is to predict the severity of delay, we thought that exploring classification capabilities was a novel step to allow us to assess the versatility of our model toward both regression and classification tasks.\n",
    "\n",
    "Ultimately, focusing back on our objective, we end up selecting our best regression model based on results from our pure validation set, train this model on the full 2015-2018 training set, and evaluate on the full 2019 test set. Depicted in this pipeline diagram is our selection of the XGBoost model, which we elaborate on in the following sections.\n",
    "\n",
    "[Preprocessing Notebook](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319512938/command/1346418319512939)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "342c69cd-e828-4bf3-b8fd-2d617823b08f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Splits and Cross Validation in Time Series\n",
    "\n",
    "Cross validation is important in a sense that the technique allows you to prevent overfitting and evaluate model performance in a more robust way than simple train-test split. Because the order of the data is a key trait for time series data, we cannot choose random samples and assign them to either the test set or the train set. We want to avoid future-looking when we train our model using time series data. \n",
    "\n",
    "There are two main techniques we can consider when implementing cross-validation. [3] \n",
    "- Time Series Split Cross-Validation\n",
    "- Blocked Cross-Validation\n",
    "\n",
    "The idea for time series split is to divide the training set into two folds at each iteration on condition that the validation set is always ahead of the training set. Using our flight and weather datasets as an example, at the first iteration, we would train the model using 2015 data points and validate on the first month of 2016 data. For the next iteration, we train on data from 2015-2016, and validate on 2017's data, and so on to the end of training set.   \n",
    "<br>\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/time_series.png?raw=true\" width=500>\n",
    "\n",
    "Unforunately with the time series split method, it can introduce leakage from future data to the model. The model can quickly observe future patterns to forecast and try to memorize them, which can cause overfitting. The blocked cross validation method, however, eliminates this risk by adding margins at two positions. It works very much like traditional cross validation except it implements an overlap between the folds. The first is between the training and validation folds in order to prevent the model from observing lag values which are used twice, once as an estimator(regressor) and another as a response. The second is between folds used at each iteration in order to prevent the model from memorizing patterns from one iteration to the next. [4]\n",
    "<br></br>\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/block_time_split.png?raw=true\" width=500>\n",
    "\n",
    "For the purpose of our data split, we took the first 4 years of the 5 years OTPW data as a training split. From 2015 up to and including the first half of 2018, we divided into 4 overlapping 13-month folds where each training fold includes ~340 days worth of data and the following ~55 days is reserved as a test fold. The second half of 2018 is reserved as a pure validation set for us to decide on a final model, trained on the 2015-first half 2018 set. Our final train/test split consists of 2015-2018 as the training set and 2019 as the test set after deciding on our final model with the pure validation set.\n",
    "\n",
    "| Data Split | Start Date | End Date | Size (Rows)\n",
    "| --- | --- | --- | --- |\n",
    "| train_fold_1 | 2015-01-01 | 2015-12-06 | 5,347,694 |\n",
    "| test_fold_1 | 2015-12-07 | 2016-01-31 | 814,075 |\n",
    "| train_fold_2 | 2015-10-19 | 2016-09-25 | 5,235,747 |\n",
    "| test_fold_2 | 2016-09-26 | 2016-11-19 | 833,258 |\n",
    "| train_fold_3 | 2016-08-07 | 2017-07-13 | 5,180,073 |\n",
    "| test_fold_3 | 2017-07-14 | 2017-09-07 | 882,056 |\n",
    "| train_fold_4 | 2017-05-25 | 2018-05-01 | 5,694,775 |\n",
    "| test_fold_4 | 2018-05-02 | 2018-06-30 | 1,203,291 |\n",
    "| pure_train | 2015-01-01 | 2018-06-30 | 20,338,085 |\n",
    "| pure_valid | 2018-07-01 | 2018-12-31 | 3,620,255 |\n",
    "| train_2015-2018 | 2015-01-01 | 2018-12-31 | 23,958,340 |\n",
    "| test_2019 | 2019-01-01 | 2019-12-31 | 7,283,985 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8beab1fe-f154-42e4-8000-ce90a6088b2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Models**\n",
    "We implemented three regression models. Each model was trained using random-search hyperparameter tuning across four cross validation-folds. We used an exponentional weighted average (weights of [0.125, 0.125, 0.25, 0.5] applied from least recent to most) on cross-validation test losses of our SMAPE and MAE evaluation metrics to judge the best hyperparameters. Using the best hyperparameters, we trained each model anew on a pure training set and evaluated on a pure validation set. \n",
    "\n",
    "In additon to our three regression models, we elected to conduct a classification analysis via a Multilayer Perceptron model (deep learning). The purpose of the classification model was to see how the model would performan in a classificaiton environment where only delays greater than 15 minutes are considered delays. See Baseline Model Result section below. \n",
    "\n",
    "##### Linear Regression - Baseline\n",
    "\n",
    "The plain vanilla linear regression model acted as our first baseline model due to is interpretability and simplicity. As such we did note conduct any hyperparameter tuning and implementing ordinary least squares regression without regularization.\n",
    "\n",
    "For our OLS algorithm, the model training took 13.65 minuts on 10 workers, each at 4 cores and 14 GB RAM. The training on our pure training set took 7.97 minutes on the same cluster specifications.\n",
    "\n",
    "\n",
    "[Linear Regression Notebook](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319523006)\n",
    "\n",
    "##### Random Forest - Regression\n",
    "We considered the random forest one of our models in an attempt to compare how this non-parametric and ensemble model would perform compared to the linear regresion. To conduct our experiments with the random forest we implemented a random hyperparameter search with the below inputs, and also noted the top performing parameters:\n",
    "| Parameter | Random Search Values | Top Performing Value |\n",
    "| --- | --- | --- | \n",
    "|numTrees\t|\t[25, 50, 75, 100]| 100 | \n",
    "|maxDepth\t|\t[5, 10, 15]| 5 | \n",
    "|minInstancesPerNode|\t[1, 5, 10]| 1 | \n",
    "|featureSubsetStrategy\t|\t[\"auto\", \"all\"]| \"auto\" | \n",
    "|minInfoGain|\t[0.0, 0.1, 0.5]| 0.1 | \n",
    "|bootstrap\t|\tTrue | True | \n",
    "\n",
    "For our Random Forest algorithm, the random grid search for hyperparameter tuning took 1.16 hours on 10 workers, each at 4 cores and 14 GB RAM. The training on our pure training set took 7 minutes on the same cluster specifications.\n",
    "\n",
    "[Random Forest Notebook](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319516665/command/1346418319516681)\n",
    "\n",
    "##### XGBoost - Regression\n",
    "The final regression model which we decided to implement was the XGBoost model. The model is known for its excellent predictive performance and is also a tree-based ensemble model. However, it is distinctly different from random forest in that XGBoosts trains weak learners sequentially. To conduct our experiments with the random forest we implemented a random hyperparamter search with the below inputs, and also noted the top performing parameters:\n",
    "\n",
    "| Parameter | Random Search Values | Top Performing Value |\n",
    "| --- | --- | --- | \n",
    "|Learning Rate \t|\t[0.1, 0.2,0.3]| 0.2 | \n",
    "|Max Depth \t|\t[5, 10, 15]| 5 | \n",
    "|Min Child Weight |\t[1,3,5]| 5 | \n",
    "|Regulization (Lambda)\t|\t[1]| 1 | \n",
    " \n",
    "For XGBoost, the random grid search for hyperparameter tuning took a total of 31.32 minutes on 10 workers, each at 4 cores and 14 GB RAM. The shorter duration compared to Random Forest is explained by having less hyperparameter combinations to search through. The training on our pure training set took 14.98 minutes with the same cluster specifications.\n",
    "\n",
    "[XGBoost Notebook](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1537154891802639/command/1537154891802640)\n",
    "\n",
    "\n",
    "##### Multi-Layer Perceptron - Classification\n",
    "The multilayer perceptron is a deep learning architecture that involves multiplications of input and weight quantities and additions by bias quantities, applying non-linear functions in a layered, successive fashion to make predictions on data separated by non-linear boundaries. In our novel study to use a multi-layer perceptron to gauge how well our top regression model would perform in a classification framework, we experimented with two different architectures.\n",
    "\n",
    "| Parameter | Choices | Top Performing Choice |\n",
    "| --- | --- | --- | \n",
    "|Hidden Layers \t|\t[[16],[16, 8]]| [16] |\n",
    "\n",
    "Our input size was 90 features with an output size of 2, leading to **90-16 Sigmoid-2 SoftMax** and **90-16 Sigmoid-8 Sigmoid-2 SoftMax** architecture choices. All other parameters were set to default (100 epochs, 0.03 learning rate, 1 x 10^-6 convergence threshold) with no grid search/cross-validation performed, as our main focus was on our regression task. We base performance off of classification F2 scores on the pure validation set, which combine precision (rate of correct delay predictions out of all delay predictions) and recall (rate of correct delay predictions out of all delays) in a manner that favors recall. The choice to use F2 is motivated by the fact that we find it more important to minimize false negatives - predicting that a delay doesn't happen when one actually does - as compared to false positives, which predict a delay happening when one does not. \n",
    "\n",
    "Interestingly, the model with one hidden layer appeared to display better behavior across training in terms of loss convergence - the loss values (in binary cross entropy loss, the standard loss function for binary classification) were lower after 100 epochs of training with one hidden layer.\n",
    "<br></br>\n",
    "<img src = \"https://github.com/sunnyshin0824/db_test/blob/main/phase_3_mlp_loss.png?raw=true\" width=1000>\n",
    "\n",
    "Our development packages had limitations in not being able to support early stopping or displaying pure validation and pure training losses with this large flights dataset side by side, so we persist for 100 epochs. Thus, our pure validation F2 will be the decider of which model to choose, which is in favor of the single hidden layer.\n",
    "\n",
    "| Hidden Layers | F2 |\n",
    "| --- | --- |\n",
    "| [16] | 0.4331\t|\n",
    "| [16, 8] | 0.3859\t|\n",
    "\n",
    "Our reporting over this classification framework shall focus on results from the single hidden layer architecture. The training of both architectures on our pure training set took a total of 24.57 minutes with the same cluster specifications.\n",
    "\n",
    "[MLP Notebook](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319523767/command/1346418319523768)\n",
    "\n",
    "---\n",
    "\n",
    "#### Regression Model Results\n",
    " <img src=\"https://github.com/sunnyshin0824/db_test/blob/main/baseline_model_results.png?raw=true\" width=700>\n",
    "\n",
    " From the above results model table we were able to extract that all models had a fairly stable SMAPE across folds. However, it seems all models struggled with data in within the second CV fold. After training the three models with their best respective hyperparameters on the pure validation set, all model generalized well to the unseen data with SMAPE's only slightly worse than their Exponential Weighted SMAPE across all CV folds. However, we do note significantly lower SMAPE in the pure training set as compared to the pure S2 2018 validation set, so there is evidence of overfitting from using the entirety of 2015-S1 2018 to train. \n",
    " \n",
    " In addition, we call out the validation SMAPE values in the 80% range and note that this is due to our shifting of negative departure delay values to 0. Our models had a massive tendency to predict nonzero delays when actual delays were zero, which causes the SMAPE value to boom to 100% for each data point this occurs. Hence, we show MAE alongside SMAPE to get a sense of how far off our predictions are in raw number of minutes, and we can see the improvement of Random Forest and XGBoost over baseline in both metrics.\n",
    " \n",
    " The XGBoost model performed the best on the pure validation set according to both SMAPE and MAE and was chosen to be used for the final test set. \n",
    "\n",
    "#### Classification Model Results\n",
    " <img src=\"https://github.com/sunnyshin0824/db_test/blob/main/classif_result_ph3_new.png?raw=true\" width=700>\n",
    "\n",
    "To provide context for the multilayer perceptron (MLP) classifier results, we set a 15 minute delay threshold to similarly categorize delays with our XGBoost model. Compared to the 1-layer MLP, the XGBoost model achieved higher recall and F2 scores, correctly predicting delays for most flights that were ultimately delayed. However, the XGBoost model's lower precision shows it incorrectly predicted delays for many on-time flights. Further analysis revealed the XGBoost errors primarily involved flights delayed between 15-20 minutes.\n",
    "\n",
    "Given the concentration of errors near the 15 minute threshold, adjusting this categorization boundary in the classification framework may improve performance. For example, using a 20 minute threshold could improve precision by avoiding the predicted 15-20 minute delays that were below the cutoff. Overall, this analysis affirmed our plan to leverage XGBoost in the final modeling of the full dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63d3482c-b326-4d47-902e-64c73c952d98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Final Models**\n",
    "##### XGBoost\n",
    "We selected the XGBoost model as our final model and fit it on 2015-2018 data to evaluate it on the 2019 dataset. The training time was 13.14 minutes on 10 workers, each at 4 minutes and 14 GB RAM. The below hyperparameters were used:\n",
    "\n",
    "| Parameter | Top Performing Value |\n",
    "| --- | --- | \n",
    "|Learning Rate \t|\t 0.2 | \n",
    "|Max Depth \t|\t 5 | \n",
    "|Min Child Weight |\t 5 | \n",
    "|Regulization (Lambda)\t|\t 1 | \n",
    "\n",
    "| Final Model | SMAPE | MAE |\n",
    "| --- | --- |  --- | \n",
    "|XGBoost \t|\t 80.3 |  25.74 | \n",
    "\n",
    "[XGBoost Notebook](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1537154891802639/command/1537154891802640)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46142769-8ca5-4b7d-b329-5cc163ca54b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Final Model Gap Analysis\n",
    "A gap analysis is a process of assessing the difference between the actual performance or results from the model and its deisred future state. It helps identify suboptimal or missing strategies, structures, capabilities, processes, practices, techniques or skills, therefore allowing ourselves to better understand and address those discrepancies in the future. \n",
    "\n",
    "The conducted gap analysis based on the models reveals some noteworthy insights into the predictive accuracy of the model, particularly in the context of estimating time delays for flight arrivals. One key metric assesses its the percentage of predictions that fall within a 5-minute window of the actual arrival time, which stands at 5.11%. While this suggests a relatively low precision in forecasting exact arrival times, the analysis also uncovers instances where predictions deviated significantly from the actual delays. In fact, the below histogram shows us that the differences in predictions obtains peak frequency at about 20 minutes (the histogram is cut at 100 minutes as higher values take low counts). Notably, there were several instances (exactly 1,395) where the model projected delays exceeding 1,000 minutes, a substantial disparity from the true delay.\n",
    "<br></br>\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/gap_analysis_hist.png?raw=true\" width=600> \n",
    "\n",
    "An interesting pattern was shown as these prolonged discrepancies were particularly prevelent in predictions related to American Airlines (AA) and SkyWest (OO) flights, with a concentration observed during holiday periods. The analysis also highlights a significant discrepancy in predicting delays of less than 15 minutes, with a notanble 60.10% of such delays inaccurately projected as equal to or exceeding 15 minutes. Conversely, the model exhibited a lower error rate of 1.68% in predicting delays of 15 minutes or more as less than 15 minutes. This information really highlights the model's strengths and weaknesses, providing valuable insights for potential enhancements to improve its predictive capabilities, especially in scenarios involving AA/OO flights and during holiday seasons.\n",
    "<br></br>\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/gap_analysis_carrier_100plus.png?raw=true\" width=600>\n",
    "\n",
    "\n",
    "Looking back with our gap analysis findings in mind, there were some practices that may have improved generalizability and test metrics.\n",
    "- Clipping severe delays from our training data, to limit our model's tendency to predict 15-20 minute delays: we explored this practice by clipping 200+ minute delays from our pure training set, yielding an improvement in pure validation SMAPE to 80.0% and MAE to 21.26 minutes.\n",
    "- Training on the timeframe our most recent fold of data instead of the entirety of 2015-2018: we explored this practice by filtering our pure training set to on or after the start date of our latest fold at 5/25/2017. This kept the pure validation SMAPE at 80.3% and slightly improved MAE to 24.64 minutes.\n",
    "\n",
    "As these opportunities were discovered in a post-mortem analysis, for integrity and unbiasedness purposes we reserve our evaluation of these new practices toward our pure validation set and avoid re-evaluating on our blind 2019 test set. However, we shall recognize clipping and training on recent, non-stale data as additional experimentation options for future time studies projects.\n",
    "\n",
    "[Gap Analysis Notebook](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319529346/command/1346418319529639)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1da7952c-ccff-445b-859a-d9454e740ede",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Performance and Scalability Concerns\n",
    "\n",
    "In a side-by-side comparison of the baseline linear regression model and the XGBoost model, visualized through blue dots representing true values and red dots denoting predictions, a notable distinction emerges. While the two sets of dots may appear similar at first glance, it is crucial to recognize that they are aligned on different X axes, with the final model evaluated on the held-out 2019 test set. The Y axes, however, remain consistent for both models. Despite the inherent non-applicability of a direct \"apples-to-apples\" comparison due to the disparate X axes, the increase in the spread of the red dots in the final model is indicative of its greater complexity. This enhancement can be attributed to factors such as the utilization of XGBoost, feature engineering, and other sophisticated techniques. The discernible deviation in the distribution of predictions underscores the nuanced improvements introduced by the more intricate model, thereby affirming its superior performance over the baseline linear regression model.\n",
    "<br></br>\n",
    "*Baseline - Linear Regression Performance*\n",
    "\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/baseline_performance.png?raw=true\" width=700>\n",
    "\n",
    "*Final Model - XGBoost Performance*\n",
    "\n",
    "<img src=\"https://github.com/sunnyshin0824/db_test/blob/main/xgb_performance.png?raw=true\" width=700>\n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "To address these discrepancies and enhance the accuracy of predictions, several recommendations can be considered. First, a recalibration of the model parameters may be necessary to fine-tune its sensitivity to different delay ranges. Also, a thorough review of the data pertaining to AA airlines on holidays could uncover specific patterns or anomalies contributing to the inaccuracies. A more nuanced approach to the prediction of delays less than 15 minutes, perhaps employing a separate model or adjusting the existing one, could mitigate the observed overestimation. Regular updates to the model using the latest data and continuous monitoring of its performance against real-world outcomes would also be essential to ensure ongoing accuracy and effectiveness.\n",
    "\n",
    "**Scalability Concerns**\n",
    "\n",
    "In terms of the scalability of this model, there are a few concerns involving the vast and dynamic datasets. As the dataset grows with an increasing number of flights and associated variables, the model's ability to process and analyze data in a timely manner becomes crucial. The computational demands of handling large datasets might strain the processing capabilities, potentially leading to delays in generating predictions. If the model were to be implemented to the real world scenarios, the need for quick response times becomes paramount. As the number of users or requests increases, the system's ability to handle concurrent predictions without latency issues could be a real concern. Additionally, if our models were to scaled up to predict flight delays globally, the scalability of the infrastructure to handle the data from diverse geographical regions and time zones is crucial, not to mention the differences in data formats, regulations, and airline practices across regions. As the usage of the model scales, monitoring for potential isues, errors and system performance becomes increasingly important. Being able to establish robust monitoring systmes and processes for proactive maintenance is going to be crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b5b7f7b-50e1-49e3-a717-9d03aa3e98cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Leakage Handling\n",
    "\n",
    "Data leakage, a malpractice in machine learning that happens when information about test data propagates into training data, could have serious consequences towards a model's generalizability toward live, unseen data in a deployment setting. Here, we reiterate and summarize the steps we took to prevent data leakage throughout our data preparation and splitting process.\n",
    "- EDA on only our training data from 2015-2018, to prevent prior knowledge of any record in the 2019 dataset.\n",
    "- Applying values of transformations of training data toward test folds/sets: our usage of min-max scaling, median imputation, and one-hot encoding are all done such that the values from our training set are applied to the test set. For instance, our min-max scaling method shifts the test set using the minimum within the training set. Our one-hot encoding schemes use the same indices from the training set toward the test set, and unseen values in the test set are put in a separate index.\n",
    "- In feature engineering, calculating features such that no future information leaks into past information: for instance, in our previous month's PageRank feature for origin airport, we calculate PageRank using the previous month's flights, shifting back another month for flights that occur on the 1st of a month before 2AM local time to honor the 2 hours' prior rule. \n",
    "  - In addition, our methodology using the Prophet tool to fit a time series model onto training data and predict toward test data, instead of trying to fit a model on training and test data combined, considers the fact that in a production setting, we cannot fit a Prophet model on future data. Hence, we use the \"additive terms\" feature as previously mentioned on the Prophet model's *forecast* of test data, only fitting on training data.\n",
    "- Train/test splits: in our cross-validation folds and train/test splits, training data is always chronologically before testing data, preventing any evaluation of past data trained on future data.\n",
    "\n",
    "However, this did not come without challenges. The gaps that we acknowledge in our process related to leakage included the following.\n",
    "- For the previous delay feature, flights that were scheduled at least two hours prior but whose delay amounted to a true departure time after two hours prior. We acknowledge this as a process gap in our feature engineering, and in future studies, we will take one of the following alternatives: \n",
    "  - Use the delay of the latest arrival time at least two hours prior with the same tail number.\n",
    "  - If using the latest scheduled departure time at least two hours prior, cap the delay number off to the difference in time between the two hours prior time and that scheduled departure time.\n",
    "- Training on the entirety of the 2015-2018 set and including stale data from 2015-2016. Though this is not directly a leakage concern in the sense that we used future information on the past, we may have overfit on stale data by training on a 4-year time frame instead of using only the most recent fold plus the pure validation set. As we already conducted our 2019 test set evaluation after training on 2015-2018, for integrity and unbiasedness purposes we shall not re-evaluate on our test set by re-training on a different subset of data, but we will take into account the drawbacks of training on stale data in future studies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ce7aade-9401-4747-a4dc-aee02ed135ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Limitations, Challenges and Future Work\n",
    "\n",
    "**Challenges**\n",
    "\n",
    "Developing a model for flight delay predictions poses a set of intricate challenges rooted in the complexity of air travel dynamics. One significant hurdle lies in the multitude of variables affecting flight schedules, encompassing weather conditions, air traffic congestion, and unforeseen technical issues. Incorporating these dynamic factors into a predictive model demands extensive data integration and preprocessing, adding layers of complexity to the development process. Additionally, the aviation industry's inherent volatility introduces unpredictability, making it challenging to establish a universal model applicable across diverse scenarios. The need for real-time data updates and the potential for rapidly changing conditions further complicates the model's accuracy. Moreover, the interdependence of various airport systems and airlines necessitates seamless data sharing and collaboration, often hindered by disparate data formats and privacy concerns. Balancing precision with interpretability in the model is another challenge, as stakeholders often require transparent insights into the decision-making process. In essence, the multifaceted nature of the aviation landscape poses considerable challenges in crafting a reliable and comprehensive model for flight delay predictions.\n",
    "\n",
    "**Future Considerations**\n",
    "\n",
    "To enhance its applicability and efficacy, future considerations should focus on deploying the model into operational systems with the latest data, spanning the years 2020-2022, to provide real-time insights to airlines. Regular updates, essential for ensuring adaptability to emerging patterns and trends in flight delays, must be prioritized. Furthermore, ongoing research and development efforts should refine the model and broaden its scopes, while engagement with regulatory bodies is crucial to align the model with industry standards and compliance requirements. By addressing these considerations, the XGBoost regression model stands poised to contribute significantly to the continuous improvement of flight delay prediction and mitigation strategies within the aviation industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3f78b13-aa11-4a20-8d26-0f2183df7d9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "In conclusion, our adopting the XGBoost regression model represented advancement in predictive performance compared to baseline linear regression. On our pure validation set of second-half 2018 flights data, XGBoost improved baseline SMAPE from 82% to 80.3% and MAE from 31.77 minutes to 24.76 minutes (a 22% reduction), maintaining similar robust results toward 2019 test data at 80.3% SMAPE and 25.74 MAE. Our best successes in feature engineering toward these improvements lied in our previous delay and seasonality factors, considering leakage to calculate over past time frames. \n",
    "\n",
    "The XGBoost regression model emerges as a robust tool, outperforming the baseline and random forest model, while exemplifying the benefits from advanced, scalable machine learning methodologies such as blocked cross-validation and random grid search of hyperparameters. Ultimately, we hope our regression study achieves its value proposition in helping airlines to help anticipate delays via factors, both in and out of our model scope, and eliminate frustration within airline customers."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "261 Final Project - Phase 3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
